[
    {
        "id": "gensim-task-0000",
        "conversations": [
            {
                "from": "human",
                "value": "You are an expert in creating robotic simulation environments and tasks. You are given some articulated assets for example. Please come up with creative use of the gripper to manipulate a single articulated object. Note that the simulation engine does not support deformable objects or accurate collision models for contacts. Moreover, the robot can only execute a simple trajectory of one motion.\n\n=========\nHere are all the assets. Please try to come up with tasks using only these assets.\n\"\"\"\n- articulated-object: {'box_rotate': {'asset_name': 'box', 'asset_description': 'A box fixed on the table, with a lid connected by a revolute joint that can be opened and closed', 'asset_class': 'Articulated Object'}, 'box_move': {'asset_name': 'box', 'asset_description': 'A box that can be moved on the table', 'asset_class': 'Articulated Object'}, 'laptop_rotate': {'asset_name': 'laptop', 'asset_description': 'A laptop fixed on the table, with a lid connected by a revolute joint that can be opened and closed', 'asset_class': 'Articulated Object'}, 'laptop_move': {'asset_name': 'laptop', 'asset_description': 'A laptop that can be moved on the table', 'asset_class': 'Articulated Object'}, 'faucet': {'asset_name': 'faucet', 'asset_description': 'A faucet fixed on the table, with a handle connected by a revolute joint that can be turned on and off', 'asset_class': 'Articulated Object'}, 'drawer': {'asset_name': 'drawer', 'asset_description': 'A drawer fixed on the table, connected with its body by a prismatic joint that can be opened and closed', 'asset_class': 'Articulated Object'}, 'door': {'asset_name': 'door', 'asset_description': 'A door with its frame connected by a revolute joint that can be opened and closed', 'asset_class': 'Articulated Object'}, 'oven': {'asset_name': 'oven', 'asset_description': 'An oven fixed on the table, with a door connected by a revolute joint that can be opened and closed', 'asset_class': 'Articulated Object'}, 'dishwasher': {'asset_name': 'dishwasher', 'asset_description': 'A dishwasher with a door connected by a revolute joint that can be opened and closed', 'asset_class': 'Articulated Object'}, 'microwave': {'asset_name': 'microwave', 'asset_description': 'A microwave fixed on the table, with a door connected by a revolute joint that can be opened and closed', 'asset_class': 'Articulated Object'}, 'refrigerator': {'asset_name': 'refrigerator', 'asset_description': 'A refrigerator fixed on the table, with a door connected by a revolute joint that can be opened and closed', 'asset_class': 'Articulated Object'}, 'safe_move': {'asset_name': 'safe', 'asset_description': 'A safe that can be moved on the table', 'asset_class': 'Articulated Object'}, 'bag_move': {'asset_name': 'bag', 'asset_description': 'A bag that can be moved on the table', 'asset_class': 'Articulated Object'}, 'bag_lift': {'asset_name': 'bag', 'asset_description': 'A bag that can be lifted from the table', 'asset_class': 'Articulated Object'}, 'suitcase_rotate': {'asset_name': 'suitcase', 'asset_description': 'A suitcase fixed on the table, with a lid connected by a revolute joint that can be opened and closed', 'asset_class': 'Articulated Object'}, 'suitcase_move': {'asset_name': 'suitcase', 'asset_description': 'A suitcase that can be moved on the table', 'asset_class': 'Articulated Object'}, 'switch': {'asset_name': 'switch', 'asset_description': 'A switch with a frame connected by a revolute joint', 'asset_class': 'Articulated Object'}, 'toaster_move': {'asset_name': 'toaster', 'asset_description': 'A toaster that can be moved on the table', 'asset_class': 'Articulated Object'}, 'washing_machine': {'asset_name': 'washing_machine', 'asset_description': 'A washing machine fixed on the table, with a door connected by a revolute joint', 'asset_class': 'Articulated Object'}, 'window_push': {'asset_name': 'window', 'asset_description': 'A window with a frame connected by a prismatic joint that can be pushed left and right', 'asset_class': 'Articulated Object'}, 'window_rotate': {'asset_name': 'window', 'asset_description': 'A window with a frame connected by a revolute joint', 'asset_class': 'Articulated Object'}, 'bucket_move': {'asset_name': 'bucket', 'asset_description': 'A bucket that can be moved on the table', 'asset_class': 'Articulated Object'}, 'bucket_lift': {'asset_name': 'bucket', 'asset_description': 'A bucket that can be lifted from the table', 'asset_class': 'Articulated Object'}}\n\n\n\n\"\"\"\n\n\n=========\nHere are some examples of good tasks. Try to be creative and high standard, and avoid overlapping with these tasks.\n\n- turn_on_faucet: {'assets-used': ['faucet'], 'task-description': 'Turn on a faucet.', 'task-name': 'turn-on-faucet'}\n- open_laptop: {'assets-used': ['laptop_rotate'], 'task-description': 'Open a laptop', 'task-name': 'open-laptop'}\n- open_box: {'assets-used': ['box_rotate'], 'task-description': 'Open a box', 'task-name': 'open-box'}\n- turn_off_faucet: {'assets-used': ['faucet'], 'task-description': 'Turn off a faucet.', 'task-name': 'turn-off-faucet'}\n- close_laptop: {'assets-used': ['laptop_rotate'], 'task-description': 'Close a laptop', 'task-name': 'close-laptop'}\n- close_box: {'assets-used': ['box_rotate'], 'task-description': 'Close a box', 'task-name': 'close-box'}\n\n\n\n\n\n=========\nHere are some bad example task instances with reasons. Try to avoid generating such tasks.\n{\n  \"task-name\": \"insert-inside-box\",\n  \"task-description\": \"Insert the gripper inside the box.\",\n  \"assets-used\": [\"box_rotate\"],\n  \"success-criteria\": [\"distance_articulated_rigidbody\"]\n}\nreasons: it's hard to define and identify the behavior of \"inserting\", which is unclear, locating is better.\n\n{\n  \"task-name\": \"toggle-drawer-openness\",\n  \"task-description\": \"Toggle the drawer's state from open to closed, or closed to open, depending on its initial state.\",\n  \"assets-used\": [\"drawer_rotate\"],\n  \"success-criteria\": [\"articulated_closed\"]\n}\nreasons: the generated task objective (\"from open to closed or closed to open\") is not clear, we need a deterministic task, e.g., we can set the specific objective as \"from open to closed\".\n\n{\n  \"task-name\": \"place-laptop-in-box\",\n  \"task-description\": \"Place the laptop in the box.\",\n  \"assets-used\": [\"laptop_move\", \"box_rotate\"],\n  \"success-criteria\": [\"distance_articulated_articulated\"]\n}\nreasons: only one articulated object can be loaded, we cannot use laptop and box at the same time.\n\n{\n  \"task-name\": \"open-laptop-and-type\",\n  \"task-description\": \"Open the laptop and type some characters.\",\n  \"assets-used\": [\"laptop_rotate\"],\n  \"success-criteria\": [\"articulated_open\"]\n}\nreasons: the robot can only execute a simple trajectory of one motion, there are two motions (opening and typing) in the task.\n\n{\n  \"task-name\": \"close-drawer-lock\",\n  \"task-description\": \"Push the drawer to a closed position and rotate the handle to a locked orientation\",\n  \"assets-used\": [\"drawer\"],\n  \"success-criteria\": [\"articulated_closed\"]\n}\nreasons: the robot can only execute a simple trajectory of one motion, there are two motions (pushing and rotating) in the task. Moreover, the drawer has only prismatic joint which cannot be rotated.\n\n=========\n\nPlease describe a new task  in natural languages and explain its novelty and challenges.\nNote:\n- Do not use assets that are not in the list above.\n- Do not repeat the tasks similar to the good examples or the already generated tasks.\n- The task needs to obey physics and remain feasible.\n- Do not create similar tasks with the same \"assets-used\" set.\n- All the assets are on the table when initialized.\n- Do not place objects on small objects.\n- Only one articulated object can be loaded.\n- The task contains a simple trajectory of one motion.\n- The task should have a clear goal, e.g. use \"open/close\" instead of \"adjust position\".\n\nBefore the next step, please check if the generated task is a bad task shown in the above examples and meet all the criteria as stated above. Specifically, if the task **only** contains a simple trajectory of one motion, and should have a **clear** goal. Explain in detail, and get a conclusion. If the task is a bad task, regenerate a new one.\n\nThen, format the answer in a Python dictionary with keys \"task-name\" and value type string, \"task-description\" (one short phrase), and value type string with lower-case and separated by hyphens, \"assets-used\" and value type list of strings, and \"success-criteria\" (choose from \"articulated_open\", \"articulated_closed\", \"distance_articulated_rigidbody\", \"distance_gripper_rigidbody\", and \"distance_gripper_articulated\") and value type list of strings. Try to be as creative as possible.\n\nPlease remember not to add any extra comments to the Python dictionary. \n\nLet's think step by step."
            },
            {
                "from": "gpt",
                "value": "Task Name: OpenDrawer\n\nDescription: The task \"OpenDrawer\" involves using the robot's gripper to pull open a drawer that is fixed on the table. This task is novel because it requires precise control of the gripper to engage with the handle of the drawer and apply a pulling force to slide the drawer open along its prismatic joint. The challenge lies in the accuracy of the gripper's positioning and the force applied to open the drawer without overshooting or applying excessive force that could result in a simulation error due to the limitations of the collision model.\n\nThe task is feasible and obeys physics as it involves a single, linear motion along the axis of the drawer's prismatic joint. The goal is clear: to open the drawer, which is a distinct and measurable outcome. This task does not overlap with the good examples provided, as none of them involve opening a drawer. It also meets the criteria of involving only one articulated object and a simple trajectory of one motion.\n\nThe success of the task can be measured using the \"articulated_open\" criterion, which would check whether the drawer has been moved from its closed position to an open position.\n\nConclusion: The task \"OpenDrawer\" is not a bad task as per the examples provided. It meets all the criteria and has a clear goal with a simple trajectory of one motion.\n\nHere is the task formatted as a Python dictionary:\n\n```python\n{\n  \"task-name\": \"open-drawer\",\n  \"task-description\": \"pull open a drawer\",\n  \"assets-used\": [\"drawer\"],\n  \"success-criteria\": [\"articulated_open\"]\n}\n```"
            }
        ]
    },
    {
        "id": "gensim-code-0000",
        "conversations": [
            {
                "from": "human",
                "value": "Now I will provide you with some reference code and you can write the code for the task \"open-drawer\".\n\n\n\"\"\"\nimport numpy as np\nimport transforms3d\n\nfrom gensim.env.base.base_task import GenSimBaseTask\nfrom gensim.env.utils.pose import *\nfrom gensim.env.utils.rewards import *\n\n\nclass CloseBox(GenSimBaseTask):\n    \"\"\"Close a box.\"\"\"\n\n    def __init__(self, env, asset_id=\"\"):\n        self.task_description = \"Close a box.\"\n        self.sub_tasks = [\"CloseBox\"]\n        self.sub_task_descriptions = [\"Close a box\"]\n        self.success_criteria = [\"articulated_closed\"]\n        # For articulated tasks, articulator should be specified, while others should be None.\n        super().__init__(env, articulator=\"box_rotate\", tool=None, tooled_object=None, asset_id=asset_id)\n\n    def reset(self, random=False):\n        super().reset()\n\n        if not random:\n            set_default_openness(self.articulator, self.articulator_name, self.asset_id)\n            set_default_pose(self.articulator, self.articulator_name, self.asset_id)\n        else:\n            set_random_pose(self.articulator, self.articulator_name, self.asset_id, self.task_type)\n            set_random_openness(self.articulator, self.articulator_name, self.asset_id)\n\n        return self.env.get_observation()\n\"\"\"\n\n\"\"\"\nimport numpy as np\nimport transforms3d\n\nfrom gensim.env.base.base_task import GenSimBaseTask\nfrom gensim.env.utils.pose import *\nfrom gensim.env.utils.rewards import l2_distance_reward, joint_fraction_reward, progress_reward, check_openness\n\n\nclass OpenLaptop(GenSimBaseTask):\n    \"\"\"Open a laptop.\"\"\"\n\n    def __init__(self, env, asset_id=\"\"):\n        self.task_description = \"Open a Laptop\"\n        self.sub_tasks = [\"OpenLaptop\"]\n        self.sub_task_descriptions = [\"Open a laptop\"]\n        self.success_criteria = [\"articulated_open\"]\n        # For articulated tasks, articulator should be specified, while others should be None.\n        super().__init__(env, articulator=\"laptop_rotate\", tool=None, tooled_object=None, asset_id=asset_id)\n\n    def reset(self, random=False):\n        super().reset()\n\n        if not random:\n            set_default_openness(self.articulator, self.articulator_name, self.asset_id)\n            set_default_pose(self.articulator, self.articulator_name, self.asset_id)\n        else:\n            set_random_pose(self.articulator, self.articulator_name, self.asset_id, self.task_type)\n            set_random_openness(self.articulator, self.articulator_name, self.asset_id)\n\n        return self.env.get_observation()\n\"\"\"\n\n\"\"\"\nimport numpy as np\nimport transforms3d\n\nfrom gensim.env.base.base_task import GenSimBaseTask\nfrom gensim.env.utils.rewards import l2_distance_reward, joint_fraction_reward, check_openness, progress_reward\nfrom gensim.env.utils.pose import *\n\n\nclass TurnOnFaucet(GenSimBaseTask):\n    \"\"\"Turn on a faucet.\"\"\"\n\n    def __init__(self, env, asset_id=\"\"):\n        self.task_description = \"Turn on a faucet.\"\n        self.sub_tasks = [\"TurnOnFaucet\"]\n        self.sub_task_descriptions = [\"Turn on a faucet\"]\n        self.success_criteria = [\"articulated_open\"]\n        # For articulated tasks, articulator should be specified, while others should be None.\n        super().__init__(env, articulator=\"faucet\", tool=None, tooled_object=None, asset_id=asset_id)     \n\n    def reset(self, random=False):\n        super().reset()\n\n        if not random:\n            set_default_openness(self.articulator, self.articulator_name, self.asset_id)\n            set_default_pose(self.articulator, self.articulator_name, self.asset_id)\n        else:\n            set_random_pose(self.articulator, self.articulator_name, self.asset_id, self.task_type)\n            set_random_openness(self.articulator, self.articulator_name, self.asset_id)\n\n        return self.env.get_observation()      \n\"\"\"\n\nThe generated code should follow the same structure as the reference code and call similar functions.\n\nDO NOT USE libraries, extra functions, properties, arguments or assets that YOU DON'T KNOW. \n\nFor the functions used, remember to import them from the corresponding files as the example task codes.\n\nFor the articulated object, use \"self.articulator\" to refer it, which should be the same as the \"assets-used\" of the task.\n\nFor the objects used, you only have to pass the corresponding parameter (e.g, articulator) and its name (string format, e.g., 'box') in the `__init__` function as shown in above codes, and the base class will automatically load them.\n\nPlease comment the code liberally to explain what each piece does and why it's written that way.\n\nNow write the code for the task \"open-drawer\" in python code block starting with ```python.  Reminder: {'task-name': 'open-drawer', 'task-description': 'pull open a drawer', 'assets-used': ['drawer'], 'success-criteria': ['articulated_open']}\n"
            },
            {
                "from": "gpt",
                "value": "```python\nimport numpy as np\nimport transforms3d\n\nfrom gensim.env.base.base_task import GenSimBaseTask\nfrom gensim.env.utils.pose import *\nfrom gensim.env.utils.rewards import *\n\nclass OpenDrawer(GenSimBaseTask):\n    \"\"\"Open a drawer.\"\"\"\n\n    def __init__(self, env, asset_id=\"\"):\n        # Define the task description and success criteria based on the task definition.\n        self.task_description = \"Pull open a drawer.\"\n        self.sub_tasks = [\"OpenDrawer\"]\n        self.sub_task_descriptions = [\"Pull open a drawer\"]\n        self.success_criteria = [\"articulated_open\"]\n        # For articulated tasks, articulator should be specified, while others should be None.\n        # The articulator is the drawer, which is the object that the robot will interact with.\n        super().__init__(env, articulator=\"drawer\", tool=None, tooled_object=None, asset_id=asset_id)\n\n    def reset(self, random=False):\n        # Reset the environment to the initial state for the task.\n        # This involves setting the pose and openness of the drawer.\n        super().reset()\n\n        if not random:\n            # Set the drawer to its default pose and openness.\n            # This is the starting condition for the task.\n            set_default_openness(self.articulator, self.articulator_name, self.asset_id)\n            set_default_pose(self.articulator, self.articulator_name, self.asset_id)\n        else:\n            # For a random start, set a random pose and openness for the drawer.\n            # This adds variability to the task for training robustness.\n            set_random_pose(self.articulator, self.articulator_name, self.asset_id, self.task_type)\n            set_random_openness(self.articulator, self.articulator_name, self.asset_id)\n\n        # Return the initial observation of the environment after reset.\n        return self.env.get_observation()\n```"
            }
        ]
    },
    {
        "id": "gensim-solver1-0000",
        "image": "gensim2/0000/env.png",
        "conversations": [
            {
                "from": "human",
                "value": "<image>\nYou are an expert in solving robotic tasks by providing some motion plans and coding task solution configs for a 2-finger robot arm. Now please solve the newly generated task by generating the task solution config. \nThe task solution config contains necessary positions, parameters and keypoints for an existing trajectory optimization algorithm to solve a feasible solution. It mainly contains three parts, constraints, pre-actuation motions and post-actuation motions:\n(1) The constraints are used to ensure the gripper to be in contact with the object and to implicitly define a certain actuation pose. An actuation pose means the key frame that the robot arm manipulate the object, usually represents the moment when the gripper get contact with the object.\n(2) The pre-actuation motions are used to move the gripper to the actuation pose without collision to the table and useless part of the object to manipulate.\n(3) The post-actuation motions are used to complete the task after the actuation pose.\n\nNow first generate 3 different configs task \"open-drawer\". \n\n=========\nHere is the task description.\n\npull open a drawer using the gripper\n\n=========\nHere are all the available keypoint names for the used manipulator and asset and their descriptions.\n\n- tool_head: the middle point between the two finger tips\n- tool_tail: the middle point between the two finger bases\n- tool_side: the tip of the right finger\n- articulated_object_head: the middle point of the the drawer handle\n- articulated_object_inside: a point to drop object inside the drawer\n\n\n\n\n=========\nHere are some examples of the constraint part of some configs.\n\n\"\"\"\ntask_name: CloseBox\ncategory_name: Articulated\n\ntool_keypoint_name_list: [tool_head, tool_tail, tool_side]\nobject_keypoint_name_list: [articulated_object_head, articulated_object_tail]\n\nconstraint_list:\n# ensure the gripper to be in contact with the box lid with whole fingers\n- keypoint_name: tool_tail\n  target_keypoint_name: articulated_object_head\n  tolerance: 0.0001\n  type: point2point_constraint\n\n# ensure surface of the gripper to be parallel to the box lid\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_side\n  target_axis: [1.0, 0, 0]\n  target_axis_frame: object\n  tolerance: 0.01\n  target_inner_product: 1\n  type: frame_axis_parallel\n\n# ensure surface of the gripper to be parallel to the box lid\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_tail\n  target_axis_from_keypoint_idx: 0\n  target_axis_from_keypoint_name: articulated_object_head\n  target_axis_to_keypoint_idx: 1\n  target_axis_to_keypoint_name: articulated_object_tail\n  tolerance: 0.01\n  target_inner_product: -1\n  type: keypoint_axis_parallel\n\n# ensure gripper to point directly to the direction of the box\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_side\n  target_axis_from_keypoint_idx: 0\n  target_axis_from_keypoint_name: articulated_object_head\n  target_axis_to_keypoint_idx: 1\n  target_axis_to_keypoint_name: articulated_object_tail\n  tolerance: 0.01\n  target_inner_product: 0\n  type: keypoint_axis_orthogonal\n\n# ensure gripper to point directly to the direction of the box\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_tail\n  target_axis: [1.0, 0, 0]\n  target_axis_frame: object\n  tolerance: 0.01\n  target_inner_product: 0\n  type: frame_axis_orthogonal\n\n\"\"\"\n\n\"\"\"\ntask_name: OpenLaptop\ncategory_name: Articulated\n\ntool_keypoint_name_list: [tool_head, tool_tail, tool_side]\nobject_keypoint_name_list: [articulated_object_head, articulated_object_tail]\n\nconstraint_list:\n# make sure the hammer head hits at the pin \n- keypoint_name: tool_tail\n  target_keypoint_name: articulated_object_head\n  tolerance: 0.0001\n  type: point2point_constraint\n\n# ensure surface of the gripper to be parallel to the laptop lid\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_side\n  target_axis: [1.0, 0, 0]\n  target_axis_frame: object\n  tolerance: 0.01\n  target_inner_product: 1\n  type: frame_axis_parallel\n\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_tail\n  target_axis_from_keypoint_name: articulated_object_head\n  target_axis_to_keypoint_name: articulated_object_tail\n  tolerance: 0.01\n  target_inner_product: -1\n  type: keypoint_axis_parallel\n\n# ensure gripper to point directly to the direction of the laptop\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_side\n  target_axis_from_keypoint_name: articulated_object_head\n  target_axis_to_keypoint_name: articulated_object_tail\n  tolerance: 0.01\n  target_inner_product: 0\n  type: keypoint_axis_orthogonal\n\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_tail\n  target_axis: [1.0, 0, 0]\n  target_axis_frame: object\n  tolerance: 0.01\n  target_inner_product: 0\n  type: frame_axis_orthogonal\n\"\"\"\n\n=========\nThe initial scene of this task is visualized in the first uploaded image. The frame axes of the gripper and the object are respectively visulized in the second and third images, where red, green and blue axes represent X, Y and Z axes respectively.\n\nNote that, in the constraint list, you need to define different items of constraint to define an actuation pose for the task. \nThere are some rules for the constraint generation:\n1. There are some pre-defined types of constraints you can use:\n  (1) point2point_constraint: This constraint is used to ensure two keypoints (\"keypoint_name\" and \"target_keypoint_name\", respectively on the tool and object) to be in contact.\n  (2) frame_axis_parallel: This constraint is used to ensure two axes (respectively on the tool and object) to be parallel. The axis on the tool is defined by a unit vector from \"axis_from_keypoint_name\" to \"axis_to_keypoint_name\", while the axis on the object is defined by \"target_axis\"([1,0,0] or [0,1,0] or [0,0,1]) which is in the coordinate frame of \"target_axis_frame\"(world or object).\n  (3) frame_axis_orthogonal: This constraint is used to ensure two axes (respectively on the tool and object) to be orthogonal. The axis on the tool is defined by a unit vector from \"axis_from_keypoint_name\" to \"axis_to_keypoint_name\", while the axis on the object is defined by \"target_axis\"([1,0,0] or [0,1,0] or [0,0,1]) which is in the coordinate frame of \"target_axis_frame\"(world or object).\n  (4) keypoint_axis_parallel:  This constraint is used to ensure two axes (respectively on the tool and object) to be parallel. The axis on the tool is defined by a unit vector from \"axis_from_keypoint_name\" to \"axis_to_keypoint_name\", while the axis on the object is defined by another unit vector from \"target_axis_from_keypoint_name\" to \"target_axis_to_keypoint_name\".\n  (5) keypoint_axis_orthogonal:  This constraint is used to ensure two axes (respectively on the tool and object) to be orthogonal. The axis on the tool is defined by a unit vector from \"axis_from_keypoint_name\" to \"axis_to_keypoint_name\", while the axis on the object is defined by another unit vector from \"target_axis_from_keypoint_name\" to \"target_axis_to_keypoint_name\".\n  The tolerance is used to define the tolerance of the constraint. \n2. The target_inner_product is used to define the inner product between the two axes. For example, if you want to ensure two axes to be parallel and of same direction, you can set the target_inner_product to 1.0. If you want to ensure two axes to be parallel and of opposite direction, you can set the target_inner_product to -1.0. If you want to ensure two axes to be orthogonal, you can set the target_inner_product to 0.0.\n3. Usually, you need to define one point2point_constraint to ensure contact and several axis constraints to adjust the actuation pose.\n4. The constraints of different configs can be diverse, but their task names should be the same. Do not use terms that you have not seen before. \n5. Z axis positive points from gripper to fingers and X axis points to the front direction in the local frame of the gripper. The frame axes of the gripper visulized in the second image, where red, green and blue axes represent X, Y and Z axes respectively.\n\n=========\nPlease output the 3 different configs in order. Each config should be in the same YAML format as the examples above.\n\nPlease first list the constraints you reason about this problem in natural languages by combining some of the following templates and filling in the blanks(< >):\n- The points <keypoint name> and <keypoint name> should coincide.\n- The axis from <keypoint name> to <keypoint name> should be <aligned / opposite / orthogonal> to the axis from <keypoint name> to <keypoint name>.\n- The axis from <keypoint name> to <keypoint name> should be <aligned / opposite / orthogonal> to the <x / y / z> axis of the object in the coordinate frame of <world / object>.\n\nThen parse the languages generated above to the required YAML format.\n\n"
            },
            {
                "from": "gpt",
                "value": "For the \"OpenDrawer\" task, we need to define the constraints that will guide the robot's gripper to the correct position and orientation for opening the drawer. Here are the constraints reasoned in natural language for three different configs:\n\nConfig 1:\n- The points tool_head and articulated_object_head should coincide to ensure the gripper is in contact with the drawer handle.\n- The axis from tool_head to tool_side should be parallel to the x axis of the object in the coordinate frame of the object to ensure proper alignment for pulling.\n- The axis from tool_head to tool_tail should be orthogonal to the y axis of the object in the coordinate frame of the object to ensure the gripper's approach does not collide with the drawer.\n\nConfig 2:\n- The points tool_side and articulated_object_head should coincide to ensure the gripper is in contact with the drawer handle.\n- The axis from tool_head to tool_tail should be aligned to the z axis of the object in the coordinate frame of world to ensure the gripper pulls directly outwards.\n\nConfig 3:\n- The points tool_tail and articulated_object_head should coincide, ensuring that the back of the gripper is in contact with the drawer handle.\n- The axis from tool_head to tool_tail should be aligned to the y axis of the object in the coordinate frame of object, providing stability when pulling the drawer.\n- The axis from tool_head to tool_side should be orthogonal to the z axis of the object in the coordinate frame of world, ensuring the gripper is flat against the drawer handle during the pull.\n\nNow, let's parse these constraints into the YAML format:\n\nConfig 1:\n```yaml\ntask_name: OpenDrawer\ncategory_name: Articulated\n\ntool_keypoint_name_list: [tool_head, tool_tail, tool_side]\nobject_keypoint_name_list: [articulated_object_head]\n\nconstraint_list:\n- keypoint_name: tool_head\n  target_keypoint_name: articulated_object_head\n  tolerance: 0.0001\n  type: point2point_constraint\n\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_side\n  target_axis: [1.0, 0, 0]\n  target_axis_frame: object\n  tolerance: 0.01\n  target_inner_product: 1\n  type: frame_axis_parallel\n\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_tail\n  target_axis: [0, 1, 0]\n  target_axis_frame: object\n  tolerance: 0.01\n  target_inner_product: 0\n  type: frame_axis_orthogonal\n```\n\nConfig 2:\n```yaml\ntask_name: OpenDrawer\ncategory_name: Articulated\n\ntool_keypoint_name_list: [tool_head, tool_tail, tool_side]\nobject_keypoint_name_list: [articulated_object_head]\n\nconstraint_list:\n- keypoint_name: tool_side\n  target_keypoint_name: articulated_object_head\n  tolerance: 0.0001\n  type: point2point_constraint\n\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_tail\n  target_axis: [0, 0, 1]\n  target_axis_frame: world\n  tolerance: 0.01\n  target_inner_product: 1\n  type: frame_axis_parallel\n```\n\nConfig 3:\n```yaml\ntask_name: OpenDrawer\ncategory_name: Articulated\n\ntool_keypoint_name_list: [tool_head, tool_tail, tool_side]\nobject_keypoint_name_list: [articulated_object_head]\n\nconstraint_list:\n- keypoint_name: tool_tail\n  target_keypoint_name: articulated_object_head\n  tolerance: 0.0001\n  type: point2point_constraint\n\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_tail\n  target_axis: [0, 1, 0]\n  target_axis_frame: object\n  tolerance: 0.01\n  target_inner_product: 1\n  type: frame_axis_parallel\n\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_side\n  target_axis: [0, 0, 1]\n  target_axis_frame: world\n  tolerance: 0.01\n  target_inner_product: 0\n  type: frame_axis_orthogonal\n```"
            }
        ]
    },
    {
        "id": "gensim-solver2-0000",
        "image": "gensim2/0000/pose.png",
        "conversations": [
            {
                "from": "human",
                "value": "<image>\nYou are an expert in solving robotic tasks by providing some motion plans and coding task solution configs for a 2-finger robot arm. Now please solve the newly generated task by generating the task solution config. \nThe task solution config contains necessary positions, parameters and keypoints for an existing trajectory optimization algorithm to solve a feasible solution. It mainly contains three parts, constraints, pre-actuation motions and post-actuation motions:\n(1) The constraints are used to ensure the gripper to be in contact with the object and to implicitly define a certain actuation pose. An actuation pose means the key frame that the robot arm manipulate the object, usually represents the moment when the gripper get contact with the object.\n(2) The pre-actuation motions are used to move the gripper to the actuation pose without collision to the table and useless part of the object to manipulate.\n(3) The post-actuation motions are used to complete the task after the actuation pose.\n\nNow that you have given the constraints part of the task solution config, you need to generate 3 different pre-actuation motions for task \"open-drawer\" following the same config format shown above based on the constraint part that is generated previously.\n\n=========\nHere is the task description.\n\nopen a drawer\n\n=========\nHere are some examples of the pre-actuation part of some task solution configs.\n\n\"\"\"\ntask_name: CloseBox\ncategory_name: Articulated\n\n# Pre-actuation motions for the manipulator. \n# Each motion is represented in the [`mode`,`value`] format, where:\n# `mode` can be only only choose from {`translate_x`, `translate_y`, `translate_z`, `rotate`}\n# `value` for `translate_x`, `translate_y` or `translate_z` is the corrdinate relative to the base center of the gripper in the `x`, 'y', 'z' axis; `value` for `rotate` is the relative radian relative to the revolute joint of the object.\n# Before generation, we should highlight that, the base is the center of the gripper, Y axis is on the plane of the gripper base and is perpendicular to the direction from the gripper to the finger, Z axis points from gripper to fingers, and X axis points out to the front direction of the plane of the gripper base.\n# Units are in meters and radians respectively.\n\n# pre-actuation pose list. \npre_actuation_motions: [[\"translate_x\", 0.06], [\"translate_z\", -0.08]] # [adjust gripper direction and approach, move gripper above lid]\n\n\n\"\"\"\n\n\"\"\"\ntask_name: OpenLaptop\ncategory_name: Articulated\n\n# Pre-actuation motions for the manipulator. \n# Each motion is represented in the [`mode`,`value`] format, where:\n# `mode` can be only only choose from {`translate_x`, `translate_y`, `translate_z`, `rotate`}\n# `value` for `translate_x`, `translate_y` or `translate_z` is the corrdinate relative to the base center of the gripper in the `x`, 'y', 'z' axis; `value` for `rotate` is the relative radian relative to the revolute joint of the object.\n# Before generation, we should highlight that, the base is the center of the gripper, Y axis is on the plane of the gripper base and is perpendicular to the direction from the gripper to the finner, Z axis points from gripper to fingers, and X axis points out to the front direction of the plane of the gripper base.\n# Units are in meters and radians respectively.\n\n# pre-actuation pose list. \npre_actuation_motions: [[\"translate_x\", -0.05], [\"translate_z\", -0.05]] # [adjust gripper direction and approach, move gripper below lid]\n\n\n\"\"\"\n\n\"\"\"\ntask_name: TurnOnFaucet\ncategory_name: Articulated\n\n# Pre-actuation motions for the manipulator. \n# Each motion is represented in the [`mode`,`value`] format, where:\n# `mode` can be only only choose from {`translate_x`, `translate_y`, `translate_z`, `rotate`}\n# `value` for `translate_x`, `translate_y` or `translate_z` is the corrdinate relative to the base center of the gripper in the `x`, 'y', 'z' axis; `value` for `rotate` is the relative radian relative to the revolute joint of the object.\n# Before generation, we should highlight that, the base is the center of the gripper, Y axis is on the plane of the gripper base and is perpendicular to the direction from the gripper to the finner, Z axis points from gripper to fingers, and X axis points out to the front direction of the plane of the gripper base.\n# Units are in meters and radians respectively.\n\n# pre-actuation pose list. \npre_actuation_motions: [[\"translate_z\", -0.05]] # [move above handle]\n\n\n\"\"\"\n\n=========\nThe initial scene of this task is visualized in the first uploaded image. The actuation pose of this task is visualized in the second uploaded image. The frame axes of the gripper and the object are respectively visulized in the second and third images, where red, green and blue axes represent X, Y and Z axes respectively.\n\nThe pre-actuation motions of different solutions can be diverse, but their task names should be the same.\nDo not use terms that you have not seen before. \nThe output should be in the YAML format with no extra text. Each config should be a list of pre-actuation motions in a separated YAML cell.\nThe diversity of the pre/post-actuation motions can be achieved by using different axes for translation.\nNotice that the pre-actuation and post-actuation motions are relative to the actuation pose and the translation motions are represented in corrdinates relative to the manipulator base. Let's think step by step, try your best to understand the job, if you can generate perfect solution configs, I will give you $10,000 tip.\n\n"
            },
            {
                "from": "gpt",
                "value": "```yaml\ntask_name: open-drawer\ncategory_name: Articulated\n\npre_actuation_motions:\n  - [\"translate_z\", -0.1] # Move gripper down closer to the drawer handle\n  - [\"translate_x\", 0.05] # Move gripper forward towards the front of the drawer\n  - [\"translate_y\", -0.02] # Move gripper to the left to align with the drawer handle\n```\n\n```yaml\ntask_name: open-drawer\ncategory_name: Articulated\n\npre_actuation_motions:\n  - [\"translate_z\", -0.1] # Move gripper down closer to the drawer handle\n  - [\"translate_x\", -0.05] # Move gripper backward to create space for the drawer to open\n  - [\"translate_y\", 0.02] # Move gripper to the right to align with the drawer handle\n```\n\n```yaml\ntask_name: open-drawer\ncategory_name: Articulated\n\npre_actuation_motions:\n  - [\"translate_y\", -0.02] # Move gripper to the left to clear any obstacles\n  - [\"translate_z\", -0.1] # Move gripper down to be on the same level as the drawer handle\n  - [\"translate_x\", 0.05] # Move gripper forward to position it in front of the drawer handle\n```"
            }
        ]
    },
    {
        "id": "gensim-solver3-0000",
        "image": "gensim2/0000/pose.png",
        "conversations": [
            {
                "from": "human",
                "value": "<image>\nYou are an expert in solving robotic tasks by providing some motion plans and coding task solution configs for a 2-finger robot arm. Now please solve the newly generated task by generating the task solution config. \nThe task solution config contains necessary positions, parameters and keypoints for an existing trajectory optimization algorithm to solve a feasible solution. It mainly contains three parts, constraints, pre-actuation motions and post-actuation motions:\n(1) The constraints are used to ensure the gripper to be in contact with the object and to implicitly define a certain actuation pose. An actuation pose means the key frame that the robot arm manipulate the object, usually represents the moment when the gripper get contact with the object.\n(2) The pre-actuation motions are used to move the gripper to the actuation pose.\n(3) The post-actuation motions are used to complete the task after the actuation pose.\n\nNow that you have given the constraints part and the pre-actuation part of the task solution config, you need to generate 3 different post-actuation motions for task \"open-drawer\" following the same config format shown above based on the constraint part that is generated previously. In this part, your task is to generate some motion plans to solve the task beginning from the actuation pose.\n\n=========\nHere is the task description.\n\nopen a fixed drawer using a simple pulling motion\n\n=========\nHere are some examples of the post-actuation part of some task solution configs.\n\n\"\"\"\ntask_name: CloseBox\ncategory_name: Articulated\n\n# Post-actuation motions for the manipulator. \n# Post-actuation motions should be in the format of a python list. Each element in the list should be a string with lower-case and separated by hyphens.\n\n# post-actuation pose list\npost_actuation_motions: [\"move-backward\"] # [pull lid down]\n\n\"\"\"\n\n\"\"\"\ntask_name: OpenLaptop\ncategory_name: Articulated\n\n# Post-actuation motions for the manipulator. \n# Post-actuation motions should be in the format of a python list. Each element in the list should be a string with lower-case and separated by hyphens.\n\n# post-actuation pose list. pushing down\npost_actuation_motions: [\"move-forward\"] # [push lid up]\n\n\"\"\"\n\n\n=====================================\nThe actuation pose of this task \"open-drawer\" is visualized in the first uploaded image. You need to start from this moment to generate some motion plans to solve the task. \n\nSome rules of such motion planning are listed here:\n1. Each step in your plan should be chosen from the following motion set: \"move-forward\", \"move-backward\", \"move-left\", \"move-right\", \"move-up\", \"move-down\". Notice that you can not use the gripper finger to grip anything.\n2. The directions in the movement are defined in the robot's coordinate system. The second uploaded image shows the robot's coordinate system.\n    - The left direction is defined as the direction aligned with the green axis in the image, while the right direction is defined as the opposite direction.\n    - The forward direction is defined as the direction aligned with the red axis in the image, while the backward direction is defined as the opposite direction.\n    - The down direction is defined as the direction aligned with the blue axis in the image, while the up direction is defined as the opposite direction.\n3. The robot arm should not collide with the table in the scene during the execution of your plan.\n4. A plan should include at most 3 steps\n5. The post-actuation motion plans of different solutions can be diverse, but their task names should be the same. Do not use terms that you have not seen before. \n6. The output should be in the YAML format with no extra text.\n\n==============================\nPlease output the 3 different configs for task \"open-drawer\" in order. Each config should be in the same YAML format as the examples above with no extra text. Each config should be a list of pre-actuation motions in a separated YAML cell."
            },
            {
                "from": "gpt",
                "value": "```yaml\ntask_name: OpenDrawer\ncategory_name: Articulated\n\n# Post-actuation motions for the manipulator.\n# Pull the gripper back to open the drawer\npost_actuation_motions:\n  - \"move-backward\"\n```\n\n```yaml\ntask_name: OpenDrawer\ncategory_name: Articulated\n\n# Post-actuation motions for the manipulator.\n# Pull the gripper back and slightly upwards to open the drawer\npost_actuation_motions:\n  - \"move-backward\"\n  - \"move-up\"\n```\n\n```yaml\ntask_name: OpenDrawer\ncategory_name: Articulated\n\n# Post-actuation motions for the manipulator.\n# Pull the gripper back and slightly to the right to open the drawer\npost_actuation_motions:\n  - \"move-backward\"\n  - \"move-right\"\n```"
            }
        ]
    },
    {
        "id": "gensim-task-0001",
        "conversations": [
            {
                "from": "human",
                "value": "You are an expert in creating robotic simulation environments and tasks. You are given some articulated assets for example. Please come up with creative use of the gripper to manipulate a single articulated object. Note that the simulation engine does not support deformable objects or accurate collision models for contacts. Moreover, the robot can only execute a simple trajectory of one motion.\n\n=========\nHere are all the assets. Please try to come up with tasks using only these assets.\n\"\"\"\n- articulated-object: {'box_rotate': {'asset_name': 'box', 'asset_description': 'A box fixed on the table, with a lid connected by a revolute joint that can be opened and closed', 'asset_class': 'Articulated Object'}, 'box_move': {'asset_name': 'box', 'asset_description': 'A box that can be moved on the table', 'asset_class': 'Articulated Object'}, 'laptop_rotate': {'asset_name': 'laptop', 'asset_description': 'A laptop fixed on the table, with a lid connected by a revolute joint that can be opened and closed', 'asset_class': 'Articulated Object'}, 'laptop_move': {'asset_name': 'laptop', 'asset_description': 'A laptop that can be moved on the table', 'asset_class': 'Articulated Object'}, 'faucet': {'asset_name': 'faucet', 'asset_description': 'A faucet fixed on the table, with a handle connected by a revolute joint that can be turned on and off', 'asset_class': 'Articulated Object'}, 'drawer': {'asset_name': 'drawer', 'asset_description': 'A drawer fixed on the table, connected with its body by a prismatic joint that can be opened and closed', 'asset_class': 'Articulated Object'}, 'door': {'asset_name': 'door', 'asset_description': 'A door with its frame connected by a revolute joint that can be opened and closed', 'asset_class': 'Articulated Object'}, 'oven': {'asset_name': 'oven', 'asset_description': 'An oven fixed on the table, with a door connected by a revolute joint that can be opened and closed', 'asset_class': 'Articulated Object'}, 'dishwasher': {'asset_name': 'dishwasher', 'asset_description': 'A dishwasher with a door connected by a revolute joint that can be opened and closed', 'asset_class': 'Articulated Object'}, 'microwave': {'asset_name': 'microwave', 'asset_description': 'A microwave fixed on the table, with a door connected by a revolute joint that can be opened and closed', 'asset_class': 'Articulated Object'}, 'refrigerator': {'asset_name': 'refrigerator', 'asset_description': 'A refrigerator fixed on the table, with a door connected by a revolute joint that can be opened and closed', 'asset_class': 'Articulated Object'}, 'safe_move': {'asset_name': 'safe', 'asset_description': 'A safe that can be moved on the table', 'asset_class': 'Articulated Object'}, 'safe_rotate': {'asset_name': 'safe', 'asset_description': 'A safe fixed on the table, with a door connected by a revolute joint that can be opened and closed', 'asset_class': 'Articulated Object'}, 'bag_move': {'asset_name': 'bag', 'asset_description': 'A bag that can be moved on the table', 'asset_class': 'Articulated Object'}, 'bag_lift': {'asset_name': 'bag', 'asset_description': 'A bag that can be lifted from the table', 'asset_class': 'Articulated Object'}, 'suitcase_rotate': {'asset_name': 'suitcase', 'asset_description': 'A suitcase fixed on the table, with a lid connected by a revolute joint that can be opened and closed', 'asset_class': 'Articulated Object'}, 'suitcase_move': {'asset_name': 'suitcase', 'asset_description': 'A suitcase that can be moved on the table', 'asset_class': 'Articulated Object'}, 'switch': {'asset_name': 'switch', 'asset_description': 'A switch with a frame connected by a revolute joint', 'asset_class': 'Articulated Object'}, 'toaster_move': {'asset_name': 'toaster', 'asset_description': 'A toaster that can be moved on the table', 'asset_class': 'Articulated Object'}, 'washing_machine': {'asset_name': 'washing_machine', 'asset_description': 'A washing machine fixed on the table, with a door connected by a revolute joint', 'asset_class': 'Articulated Object'}, 'window_push': {'asset_name': 'window', 'asset_description': 'A window with a frame connected by a prismatic joint that can be pushed left and right', 'asset_class': 'Articulated Object'}, 'window_rotate': {'asset_name': 'window', 'asset_description': 'A window with a frame connected by a revolute joint', 'asset_class': 'Articulated Object'}, 'bucket_move': {'asset_name': 'bucket', 'asset_description': 'A bucket that can be moved on the table', 'asset_class': 'Articulated Object'}, 'bucket_lift': {'asset_name': 'bucket', 'asset_description': 'A bucket that can be lifted from the table', 'asset_class': 'Articulated Object'}}\n\n\n\n\"\"\"\n\n\n=========\nHere are some examples of good tasks. Try to be creative and high standard, and avoid overlapping with these tasks.\n\n- turn_on_faucet: {'assets-used': ['faucet'], 'task-description': 'Turn on a faucet.', 'task-name': 'turn-on-faucet'}\n- open_laptop: {'assets-used': ['laptop_rotate'], 'task-description': 'Open a laptop', 'task-name': 'open-laptop'}\n- open_box: {'assets-used': ['box_rotate'], 'task-description': 'Open a box', 'task-name': 'open-box'}\n- turn_off_faucet: {'assets-used': ['faucet'], 'task-description': 'Turn off a faucet.', 'task-name': 'turn-off-faucet'}\n- close_laptop: {'assets-used': ['laptop_rotate'], 'task-description': 'Close a laptop', 'task-name': 'close-laptop'}\n- close_box: {'assets-used': ['box_rotate'], 'task-description': 'Close a box', 'task-name': 'close-box'}\n\n\n\n\n\n=========\nHere are some bad example task instances with reasons. Try to avoid generating such tasks.\n{\n  \"task-name\": \"insert-inside-box\",\n  \"task-description\": \"Insert the gripper inside the box.\",\n  \"assets-used\": [\"box_rotate\"],\n  \"success-criteria\": [\"distance_articulated_rigidbody\"]\n}\nreasons: it's hard to define and identify the behavior of \"inserting\", which is unclear, locating is better.\n\n{\n  \"task-name\": \"toggle-drawer-openness\",\n  \"task-description\": \"Toggle the drawer's state from open to closed, or closed to open, depending on its initial state.\",\n  \"assets-used\": [\"drawer_rotate\"],\n  \"success-criteria\": [\"articulated_closed\"]\n}\nreasons: the generated task objective (\"from open to closed or closed to open\") is not clear, we need a deterministic task, e.g., we can set the specific objective as \"from open to closed\".\n\n{\n  \"task-name\": \"place-laptop-in-box\",\n  \"task-description\": \"Place the laptop in the box.\",\n  \"assets-used\": [\"laptop_move\", \"box_rotate\"],\n  \"success-criteria\": [\"distance_articulated_articulated\"]\n}\nreasons: only one articulated object can be loaded, we cannot use laptop and box at the same time.\n\n{\n  \"task-name\": \"open-laptop-and-type\",\n  \"task-description\": \"Open the laptop and type some characters.\",\n  \"assets-used\": [\"laptop_rotate\"],\n  \"success-criteria\": [\"articulated_open\"]\n}\nreasons: the robot can only execute a simple trajectory of one motion, there are two motions (opening and typing) in the task.\n\n{\n  \"task-name\": \"close-drawer-lock\",\n  \"task-description\": \"Push the drawer to a closed position and rotate the handle to a locked orientation\",\n  \"assets-used\": [\"drawer\"],\n  \"success-criteria\": [\"articulated_closed\"]\n}\nreasons: the robot can only execute a simple trajectory of one motion, there are two motions (pushing and rotating) in the task. Moreover, the drawer has only prismatic joint which cannot be rotated.\n\n=========\n\nPlease describe a new task named CloseSafe in natural languages and explain its novelty and challenges.\nNote:\n- Do not use assets that are not in the list above.\n- Do not repeat the tasks similar to the good examples or the already generated tasks.\n- The task needs to obey physics and remain feasible.\n- Do not create similar tasks with the same \"assets-used\" set.\n- All the assets are on the table when initialized.\n- Do not place objects on small objects.\n- Only one articulated object can be loaded.\n- The task contains a simple trajectory of one motion.\n- The task should have a clear goal, e.g. use \"open/close\" instead of \"adjust position\".\n\nBefore the next step, please check if the generated task is a bad task shown in the above examples and meet all the criteria as stated above. Specifically, if the task **only** contains a simple trajectory of one motion, and should have a **clear** goal. Explain in detail, and get a conclusion. If the task is a bad task, regenerate a new one.\n\nThen, format the answer in a Python dictionary with keys \"task-name\" and value type string, \"task-description\" (one short phrase), and value type string with lower-case and separated by hyphens, \"assets-used\" and value type list of strings, and \"success-criteria\" (choose from \"articulated_open\", \"articulated_closed\", \"distance_articulated_rigidbody\", \"distance_gripper_rigidbody\", and \"distance_gripper_articulated\") and value type list of strings. Try to be as creative as possible.\n\nPlease remember not to add any extra comments to the Python dictionary. \n\nLet's think step by step."
            },
            {
                "from": "gpt",
                "value": "Given the constraints and the assets available, the task I propose is \"CloseSafe\". This task involves the robot using its gripper to close the door of a safe. The safe is an articulated object that can be moved on the table, and for this task, it would have a door that needs to be pushed to a closed position. The novelty of this task lies in the precision required to align the gripper with the door's handle or edge and apply the correct amount of force to close it without pushing the safe away, given that the safe can move. This task challenges the robot's ability to perform precise and controlled movements.\n\nChecking against the criteria:\n- The task uses only one articulated object from the list, which is \"safe_move\".\n- The task is not similar to the good examples or the already generated tasks.\n- The task obeys physics and remains feasible.\n- The task contains a simple trajectory of one motion.\n- The goal of the task is clear: to close the safe.\n\nThe task is not a bad task because it meets all the criteria and avoids the pitfalls of the bad examples. Specifically, it involves only one motion (closing the safe), and the goal is clear (to close the safe).\n\nHere is the task formatted as a Python dictionary:\n\n```python\n{\n  \"task-name\": \"close-safe\",\n  \"task-description\": \"Close the door of a safe\",\n  \"assets-used\": [\"safe_rotate\"],\n  \"success-criteria\": [\"articulated_closed\"]\n}\n```"
            }
        ]
    },
    {
        "id": "gensim-code-0001",
        "conversations": [
            {
                "from": "human",
                "value": "Now I will provide you with some reference code and you can write the code for the task \"close-safe\".\n\n\n\"\"\"\nimport numpy as np\nimport transforms3d\n\nfrom gensim.env.base.base_task import GenSimBaseTask\nfrom gensim.env.utils.pose import *\nfrom gensim.env.utils.rewards import *\n\n\nclass CloseBox(GenSimBaseTask):\n    \"\"\"Close a box.\"\"\"\n\n    def __init__(self, env, asset_id=\"\"):\n        self.task_description = \"Close a box.\"\n        self.sub_tasks = [\"CloseBox\"]\n        self.sub_task_descriptions = [\"Close a box\"]\n        self.success_criteria = [\"articulated_closed\"]\n        # For articulated tasks, articulator should be specified, while others should be None.\n        super().__init__(env, articulator=\"box_rotate\", tool=None, tooled_object=None, asset_id=asset_id)\n\n    def reset(self, random=False):\n        super().reset()\n\n        if not random:\n            set_default_openness(self.articulator, self.articulator_name, self.asset_id)\n            set_default_pose(self.articulator, self.articulator_name, self.asset_id)\n        else:\n            set_random_pose(self.articulator, self.articulator_name, self.asset_id, self.task_type)\n            set_random_openness(self.articulator, self.articulator_name, self.asset_id)\n\n        return self.env.get_observation()\n\"\"\"\n\n\"\"\"\nimport numpy as np\nimport transforms3d\n\nfrom gensim.env.base.base_task import GenSimBaseTask\nfrom gensim.env.utils.pose import *\nfrom gensim.env.utils.rewards import l2_distance_reward, joint_fraction_reward, progress_reward, check_openness\n\n\nclass OpenLaptop(GenSimBaseTask):\n    \"\"\"Open a laptop.\"\"\"\n\n    def __init__(self, env, asset_id=\"\"):\n        self.task_description = \"Open a Laptop\"\n        self.sub_tasks = [\"OpenLaptop\"]\n        self.sub_task_descriptions = [\"Open a laptop\"]\n        self.success_criteria = [\"articulated_open\"]\n        # For articulated tasks, articulator should be specified, while others should be None.\n        super().__init__(env, articulator=\"laptop_rotate\", tool=None, tooled_object=None, asset_id=asset_id)\n\n    def reset(self, random=False):\n        super().reset()\n\n        if not random:\n            set_default_openness(self.articulator, self.articulator_name, self.asset_id)\n            set_default_pose(self.articulator, self.articulator_name, self.asset_id)\n        else:\n            set_random_pose(self.articulator, self.articulator_name, self.asset_id, self.task_type)\n            set_random_openness(self.articulator, self.articulator_name, self.asset_id)\n\n        return self.env.get_observation()\n\"\"\"\n\n\"\"\"\nimport numpy as np\nimport transforms3d\n\nfrom gensim.env.base.base_task import GenSimBaseTask\nfrom gensim.env.utils.rewards import l2_distance_reward, joint_fraction_reward, check_openness, progress_reward\nfrom gensim.env.utils.pose import *\n\n\nclass TurnOnFaucet(GenSimBaseTask):\n    \"\"\"Turn on a faucet.\"\"\"\n\n    def __init__(self, env, asset_id=\"\"):\n        self.task_description = \"Turn on a faucet.\"\n        self.sub_tasks = [\"TurnOnFaucet\"]\n        self.sub_task_descriptions = [\"Turn on a faucet\"]\n        self.success_criteria = [\"articulated_open\"]\n        # For articulated tasks, articulator should be specified, while others should be None.\n        super().__init__(env, articulator=\"faucet\", tool=None, tooled_object=None, asset_id=asset_id)     \n\n    def reset(self, random=False):\n        super().reset()\n\n        if not random:\n            set_default_openness(self.articulator, self.articulator_name, self.asset_id)\n            set_default_pose(self.articulator, self.articulator_name, self.asset_id)\n        else:\n            set_random_pose(self.articulator, self.articulator_name, self.asset_id, self.task_type)\n            set_random_openness(self.articulator, self.articulator_name, self.asset_id)\n\n        return self.env.get_observation()      \n\"\"\"\n\nThe generated code should follow the same structure as the reference code and call similar functions.\n\nDO NOT USE libraries, extra functions, properties, arguments or assets that YOU DON'T KNOW. \n\nFor the functions used, remember to import them from the corresponding files as the example task codes.\n\nFor the articulated object, use \"self.articulator\" to refer it, which should be the same as the \"assets-used\" of the task.\n\nFor the objects used, you only have to pass the corresponding parameter (e.g, articulator) and its name (string format, e.g., 'box') in the `__init__` function as shown in above codes, and the base class will automatically load them.\n\nPlease comment the code liberally to explain what each piece does and why it's written that way.\n\nNow write the code for the task \"close-safe\" in python code block starting with ```python.  Reminder: {'task-name': 'close-safe', 'task-description': 'Close the door of a safe', 'assets-used': ['safe_rotate'], 'success-criteria': ['articulated_closed']}\n"
            },
            {
                "from": "gpt",
                "value": "```python\nimport numpy as np\nimport transforms3d\n\nfrom gensim.env.base.base_task import GenSimBaseTask\nfrom gensim.env.utils.pose import *\nfrom gensim.env.utils.rewards import *\n\nclass CloseSafe(GenSimBaseTask):\n    \"\"\"Close the door of a safe.\"\"\"\n\n    def __init__(self, env, asset_id=\"\"):\n        # Define the task description and success criteria based on the task definition.\n        self.task_description = \"Close the door of a safe.\"\n        self.sub_tasks = [\"CloseSafe\"]\n        self.sub_task_descriptions = [\"Close the door of a safe\"]\n        self.success_criteria = [\"articulated_closed\"]\n        # For articulated tasks, articulator should be specified, while others should be None.\n        # The articulator is the 'safe_rotate' asset which represents the safe with a movable door.\n        super().__init__(env, articulator=\"safe_rotate\", tool=None, tooled_object=None, asset_id=asset_id)\n\n    def reset(self, random=False):\n        # Reset the environment to the initial state for the task.\n        super().reset()\n\n        # If not random, set the safe to its default pose and openness (closed state).\n        # Otherwise, set a random pose and openness for the safe.\n        if not random:\n            set_default_openness(self.articulator, self.articulator_name, self.asset_id)\n            set_default_pose(self.articulator, self.articulator_name, self.asset_id)\n        else:\n            set_random_pose(self.articulator, self.articulator_name, self.asset_id, self.task_type)\n            set_random_openness(self.articulator, self.articulator_name, self.asset_id)\n\n        # Return the initial observation of the environment after reset.\n        return self.env.get_observation()\n```"
            }
        ]
    },
    {
        "id": "gensim-solver1-0001",
        "image": "gensim2/0001/env.png",
        "conversations": [
            {
                "from": "human",
                "value": "<image>\nYou are an expert in solving robotic tasks by providing some motion plans and coding task solution configs for a 2-finger robot arm. Now please solve the newly generated task by generating the task solution config. \nThe task solution config contains necessary positions, parameters and keypoints for an existing trajectory optimization algorithm to solve a feasible solution. It mainly contains three parts, constraints, pre-actuation motions and post-actuation motions:\n(1) The constraints are used to ensure the gripper to be in contact with the object and to implicitly define a certain actuation pose. An actuation pose means the key frame that the robot arm manipulate the object, usually represents the moment when the gripper get contact with the object.\n(2) The pre-actuation motions are used to move the gripper to the actuation pose without collision to the table and useless part of the object to manipulate.\n(3) The post-actuation motions are used to complete the task after the actuation pose.\n\nNow first generate 3 different configs task \"close-safe\". \n\n=========\nHere is the task description.\n\nclose a safe door\n\n=========\nHere are all the available keypoint names for the used manipulator and asset and their descriptions.\n\n- tool_head: the middle point between the two finger tips\n- tool_tail: the middle point between the two finger bases\n- tool_side: the tip of the right finger\n- articulated_object_head: the middle point of the the safe handle\n- articulated_object_inside_base: the point inside the safe to drop objects\n\n\n\n\n=========\nHere are some examples of the constraint part of some configs.\n\n\"\"\"\ntask_name: CloseBox\ncategory_name: Articulated\n\ntool_keypoint_name_list: [tool_head, tool_tail, tool_side]\nobject_keypoint_name_list: [articulated_object_head, articulated_object_tail]\n\nconstraint_list:\n# ensure the gripper to be in contact with the box lid with whole fingers\n- keypoint_name: tool_tail\n  target_keypoint_name: articulated_object_head\n  tolerance: 0.0001\n  type: point2point_constraint\n\n# ensure surface of the gripper to be parallel to the box lid\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_side\n  target_axis: [1.0, 0, 0]\n  target_axis_frame: object\n  tolerance: 0.01\n  target_inner_product: 1\n  type: frame_axis_parallel\n\n# ensure surface of the gripper to be parallel to the box lid\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_tail\n  target_axis_from_keypoint_idx: 0\n  target_axis_from_keypoint_name: articulated_object_head\n  target_axis_to_keypoint_idx: 1\n  target_axis_to_keypoint_name: articulated_object_tail\n  tolerance: 0.01\n  target_inner_product: -1\n  type: keypoint_axis_parallel\n\n# ensure gripper to point directly to the direction of the box\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_side\n  target_axis_from_keypoint_idx: 0\n  target_axis_from_keypoint_name: articulated_object_head\n  target_axis_to_keypoint_idx: 1\n  target_axis_to_keypoint_name: articulated_object_tail\n  tolerance: 0.01\n  target_inner_product: 0\n  type: keypoint_axis_orthogonal\n\n# ensure gripper to point directly to the direction of the box\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_tail\n  target_axis: [1.0, 0, 0]\n  target_axis_frame: object\n  tolerance: 0.01\n  target_inner_product: 0\n  type: frame_axis_orthogonal\n\n\"\"\"\n\n\"\"\"\ntask_name: OpenLaptop\ncategory_name: Articulated\n\ntool_keypoint_name_list: [tool_head, tool_tail, tool_side]\nobject_keypoint_name_list: [articulated_object_head, articulated_object_tail]\n\nconstraint_list:\n# make sure the hammer head hits at the pin \n- keypoint_name: tool_tail\n  target_keypoint_name: articulated_object_head\n  tolerance: 0.0001\n  type: point2point_constraint\n\n# ensure surface of the gripper to be parallel to the laptop lid\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_side\n  target_axis: [1.0, 0, 0]\n  target_axis_frame: object\n  tolerance: 0.01\n  target_inner_product: 1\n  type: frame_axis_parallel\n\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_tail\n  target_axis_from_keypoint_name: articulated_object_head\n  target_axis_to_keypoint_name: articulated_object_tail\n  tolerance: 0.01\n  target_inner_product: -1\n  type: keypoint_axis_parallel\n\n# ensure gripper to point directly to the direction of the laptop\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_side\n  target_axis_from_keypoint_name: articulated_object_head\n  target_axis_to_keypoint_name: articulated_object_tail\n  tolerance: 0.01\n  target_inner_product: 0\n  type: keypoint_axis_orthogonal\n\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_tail\n  target_axis: [1.0, 0, 0]\n  target_axis_frame: object\n  tolerance: 0.01\n  target_inner_product: 0\n  type: frame_axis_orthogonal\n\"\"\"\n\n=========\nThe initial scene of this task is visualized in the first uploaded image. The frame axes of the gripper and the object are respectively visulized in the second and third images, where red, green and blue axes represent X, Y and Z axes respectively.\n\nNote that, in the constraint list, you need to define different items of constraint to define an actuation pose for the task. \nThere are some rules for the constraint generation:\n1. There are some pre-defined types of constraints you can use:\n  (1) point2point_constraint: This constraint is used to ensure two keypoints (\"keypoint_name\" and \"target_keypoint_name\", respectively on the tool and object) to be in contact.\n  (2) frame_axis_parallel: This constraint is used to ensure two axes (respectively on the tool and object) to be parallel. The axis on the tool is defined by a unit vector from \"axis_from_keypoint_name\" to \"axis_to_keypoint_name\", while the axis on the object is defined by \"target_axis\"([1,0,0] or [0,1,0] or [0,0,1]) which is in the coordinate frame of \"target_axis_frame\"(world or object).\n  (3) frame_axis_orthogonal: This constraint is used to ensure two axes (respectively on the tool and object) to be orthogonal. The axis on the tool is defined by a unit vector from \"axis_from_keypoint_name\" to \"axis_to_keypoint_name\", while the axis on the object is defined by \"target_axis\"([1,0,0] or [0,1,0] or [0,0,1]) which is in the coordinate frame of \"target_axis_frame\"(world or object).\n  (4) keypoint_axis_parallel:  This constraint is used to ensure two axes (respectively on the tool and object) to be parallel. The axis on the tool is defined by a unit vector from \"axis_from_keypoint_name\" to \"axis_to_keypoint_name\", while the axis on the object is defined by another unit vector from \"target_axis_from_keypoint_name\" to \"target_axis_to_keypoint_name\".\n  (5) keypoint_axis_orthogonal:  This constraint is used to ensure two axes (respectively on the tool and object) to be orthogonal. The axis on the tool is defined by a unit vector from \"axis_from_keypoint_name\" to \"axis_to_keypoint_name\", while the axis on the object is defined by another unit vector from \"target_axis_from_keypoint_name\" to \"target_axis_to_keypoint_name\".\n  The tolerance is used to define the tolerance of the constraint. \n2. The target_inner_product is used to define the inner product between the two axes. For example, if you want to ensure two axes to be parallel and of same direction, you can set the target_inner_product to 1.0. If you want to ensure two axes to be parallel and of opposite direction, you can set the target_inner_product to -1.0. If you want to ensure two axes to be orthogonal, you can set the target_inner_product to 0.0.\n3. Usually, you need to define one point2point_constraint to ensure contact and several axis constraints to adjust the actuation pose.\n4. The constraints of different configs can be diverse, but their task names should be the same. Do not use terms that you have not seen before. \n5. Z axis positive points from gripper to fingers and X axis points to the front direction in the local frame of the gripper. The frame axes of the gripper visulized in the second image, where red, green and blue axes represent X, Y and Z axes respectively.\n\n=========\nPlease output the 3 different configs in order. Each config should be in the same YAML format as the examples above.\n\nPlease first list the constraints you reason about this problem in natural languages by combining some of the following templates and filling in the blanks(< >):\n- The points <keypoint name> and <keypoint name> should coincide.\n- The axis from <keypoint name> to <keypoint name> should be <aligned / opposite / orthogonal> to the axis from <keypoint name> to <keypoint name>.\n- The axis from <keypoint name> to <keypoint name> should be <aligned / opposite / orthogonal> to the <x / y / z> axis of the object in the coordinate frame of <world / object>.\n\nThen parse the languages generated above to the required YAML format.\n\n"
            },
            {
                "from": "gpt",
                "value": "Config 1 Constraints:\n- The points tool_head and articulated_object_head should coincide.\n- The axis from tool_head to tool_side should be aligned to the x axis of the object in the coordinate frame of the object.\n- The axis from tool_head to tool_tail should be orthogonal to the z axis of the object in the coordinate frame of the object.\n\nConfig 2 Constraints:\n- The points tool_tail and articulated_object_head should coincide.\n- The axis from tool_head to tool_tail should be aligned to the y axis of the object in the coordinate frame of the object.\n- The axis from tool_head to tool_side should be orthogonal to the z axis of the object in the coordinate frame of the object.\n\nConfig 3 Constraints:\n- The points tool_side and articulated_object_head should coincide.\n- The axis from tool_head to tool_tail should be opposite to the y axis of the object in the coordinate frame of the object.\n- The axis from tool_head to tool_side should be orthogonal to the x axis of the object in the coordinate frame of world.\n\nNow, let's parse these constraints to the YAML format:\n\nConfig 1:\n```yaml\ntask_name: CloseSafe\ncategory_name: Articulated\n\ntool_keypoint_name_list: [tool_head, tool_tail, tool_side]\nobject_keypoint_name_list: [articulated_object_head]\n\nconstraint_list:\n- keypoint_name: tool_head\n  target_keypoint_name: articulated_object_head\n  tolerance: 0.0001\n  type: point2point_constraint\n\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_side\n  target_axis: [1.0, 0, 0]\n  target_axis_frame: object\n  tolerance: 0.01\n  target_inner_product: 1\n  type: frame_axis_parallel\n\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_tail\n  target_axis: [0, 0, 1]\n  target_axis_frame: object\n  tolerance: 0.01\n  target_inner_product: 0\n  type: frame_axis_orthogonal\n```\n\nConfig 2:\n```yaml\ntask_name: CloseSafe\ncategory_name: Articulated\n\ntool_keypoint_name_list: [tool_head, tool_tail, tool_side]\nobject_keypoint_name_list: [articulated_object_head]\n\nconstraint_list:\n- keypoint_name: tool_tail\n  target_keypoint_name: articulated_object_head\n  tolerance: 0.0001\n  type: point2point_constraint\n\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_tail\n  target_axis: [0, 1.0, 0]\n  target_axis_frame: object\n  tolerance: 0.01\n  target_inner_product: 1\n  type: frame_axis_parallel\n\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_side\n  target_axis: [0, 0, 1]\n  target_axis_frame: object\n  tolerance: 0.01\n  target_inner_product: 0\n  type: frame_axis_orthogonal\n```\n\nConfig 3:\n```yaml\ntask_name: CloseSafe\ncategory_name: Articulated\n\ntool_keypoint_name_list: [tool_head, tool_tail, tool_side]\nobject_keypoint_name_list: [articulated_object_head]\n\nconstraint_list:\n- keypoint_name: tool_side\n  target_keypoint_name: articulated_object_head\n  tolerance: 0.0001\n  type: point2point_constraint\n\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_tail\n  target_axis: [0, 1.0, 0]\n  target_axis_frame: object\n  tolerance: 0.01\n  target_inner_product: -1\n  type: frame_axis_parallel\n\n- axis_from_keypoint_name: tool_head\n  axis_to_keypoint_name: tool_side\n  target_axis: [1.0, 0, 0]\n  target_axis_frame: world\n  tolerance: 0.01\n  target_inner_product: 0\n  type: frame_axis_orthogonal\n```\n\nThese constraints define the actuation poses for the robot arm to close the safe door in various ways. Each configuration offers a different approach to accomplish the task, considering the points and axis alignments to ensure proper contact and motion."
            }
        ]
    },
    {
        "id": "gensim-solver2-0001",
        "image": "gensim2/0001/pose.png",
        "conversations": [
            {
                "from": "human",
                "value": "<image>\nYou are an expert in solving robotic tasks by providing some motion plans and coding task solution configs for a 2-finger robot arm. Now please solve the newly generated task by generating the task solution config. \nThe task solution config contains necessary positions, parameters and keypoints for an existing trajectory optimization algorithm to solve a feasible solution. It mainly contains three parts, constraints, pre-actuation motions and post-actuation motions:\n(1) The constraints are used to ensure the gripper to be in contact with the object and to implicitly define a certain actuation pose. An actuation pose means the key frame that the robot arm manipulate the object, usually represents the moment when the gripper get contact with the object.\n(2) The pre-actuation motions are used to move the gripper to the actuation pose without collision to the table and useless part of the object to manipulate.\n(3) The post-actuation motions are used to complete the task after the actuation pose.\n\nNow that you have given the constraints part of the task solution config, you need to generate 3 different pre-actuation motions for task \"close-safe\" following the same config format shown above based on the constraint part that is generated previously.\n\n=========\nHere is the task description.\n\nclose a safe door\n\n=========\nHere are some examples of the pre-actuation part of some task solution configs.\n\n\"\"\"\ntask_name: CloseBox\ncategory_name: Articulated\n\n# Pre-actuation motions for the manipulator. \n# Each motion is represented in the [`mode`,`value`] format, where:\n# `mode` can be only only choose from {`translate_x`, `translate_y`, `translate_z`, `rotate`}\n# `value` for `translate_x`, `translate_y` or `translate_z` is the corrdinate relative to the base center of the gripper in the `x`, 'y', 'z' axis; `value` for `rotate` is the relative radian relative to the revolute joint of the object.\n# Before generation, we should highlight that, the base is the center of the gripper, Y axis is on the plane of the gripper base and is perpendicular to the direction from the gripper to the finger, Z axis points from gripper to fingers, and X axis points out to the front direction of the plane of the gripper base.\n# Units are in meters and radians respectively.\n\n# pre-actuation pose list. \npre_actuation_motions: [[\"translate_x\", 0.06], [\"translate_z\", -0.08]] # [adjust gripper direction and approach, move gripper above lid]\n\n\n\"\"\"\n\n\"\"\"\ntask_name: OpenLaptop\ncategory_name: Articulated\n\n# Pre-actuation motions for the manipulator. \n# Each motion is represented in the [`mode`,`value`] format, where:\n# `mode` can be only only choose from {`translate_x`, `translate_y`, `translate_z`, `rotate`}\n# `value` for `translate_x`, `translate_y` or `translate_z` is the corrdinate relative to the base center of the gripper in the `x`, 'y', 'z' axis; `value` for `rotate` is the relative radian relative to the revolute joint of the object.\n# Before generation, we should highlight that, the base is the center of the gripper, Y axis is on the plane of the gripper base and is perpendicular to the direction from the gripper to the finner, Z axis points from gripper to fingers, and X axis points out to the front direction of the plane of the gripper base.\n# Units are in meters and radians respectively.\n\n# pre-actuation pose list. \npre_actuation_motions: [[\"translate_x\", -0.05], [\"translate_z\", -0.05]] # [adjust gripper direction and approach, move gripper below lid]\n\n\n\"\"\"\n\n\"\"\"\ntask_name: TurnOnFaucet\ncategory_name: Articulated\n\n# Pre-actuation motions for the manipulator. \n# Each motion is represented in the [`mode`,`value`] format, where:\n# `mode` can be only only choose from {`translate_x`, `translate_y`, `translate_z`, `rotate`}\n# `value` for `translate_x`, `translate_y` or `translate_z` is the corrdinate relative to the base center of the gripper in the `x`, 'y', 'z' axis; `value` for `rotate` is the relative radian relative to the revolute joint of the object.\n# Before generation, we should highlight that, the base is the center of the gripper, Y axis is on the plane of the gripper base and is perpendicular to the direction from the gripper to the finner, Z axis points from gripper to fingers, and X axis points out to the front direction of the plane of the gripper base.\n# Units are in meters and radians respectively.\n\n# pre-actuation pose list. \npre_actuation_motions: [[\"translate_z\", -0.05]] # [move above handle]\n\n\n\"\"\"\n\n=========\nThe initial scene of this task is visualized in the first uploaded image. The actuation pose of this task is visualized in the second uploaded image. The frame axes of the gripper and the object are respectively visulized in the second and third images, where red, green and blue axes represent X, Y and Z axes respectively.\n\nThe pre-actuation motions of different solutions can be diverse, but their task names should be the same.\nDo not use terms that you have not seen before. \nThe output should be in the YAML format with no extra text. Each config should be a list of pre-actuation motions in a separated YAML cell.\nThe diversity of the pre/post-actuation motions can be achieved by using different axes for translation.\nNotice that the pre-actuation and post-actuation motions are relative to the actuation pose and the translation motions are represented in corrdinates relative to the manipulator base. Let's think step by step, try your best to understand the job, if you can generate perfect solution configs, I will give you $10,000 tip.\n\n"
            },
            {
                "from": "gpt",
                "value": "```yaml\ntask_name: CloseSafe\ncategory_name: Articulated\n\n# Pre-actuation motions for the manipulator. \n# Approach from the front and slightly above the safe door handle\npre_actuation_motions: \n  - [\"translate_z\", -0.1] # Move gripper down to be level with safe handle\n  - [\"translate_x\", 0.05] # Move gripper forward towards the safe\n```\n\n```yaml\ntask_name: CloseSafe\ncategory_name: Articulated\n\n# Pre-actuation motions for the manipulator. \n# Approach from the side and above the safe door handle\npre_actuation_motions: \n  - [\"translate_y\", -0.1] # Move gripper to the side in line with the safe handle\n  - [\"translate_z\", -0.1] # Move gripper down to the level of the safe handle\n```\n\n```yaml\ntask_name: CloseSafe\ncategory_name: Articulated\n\n# Pre-actuation motions for the manipulator. \n# Approach diagonally above the safe\npre_actuation_motions: \n  - [\"translate_z\", -0.07] # Move gripper down to a higher level than the safe handle\n  - [\"translate_x\", 0.03] # Move gripper forward and closer to the safe handle\n  - [\"translate_y\", -0.05] # Move gripper to the side towards the safe handle\n```"
            }
        ]
    },
    {
        "id": "gensim-solver3-0001",
        "image": "gensim2/0001/pose.png",
        "conversations": [
            {
                "from": "human",
                "value": "<image>\nYou are an expert in solving robotic tasks by providing some motion plans and coding task solution configs for a 2-finger robot arm. Now please solve the newly generated task by generating the task solution config. \nThe task solution config contains necessary positions, parameters and keypoints for an existing trajectory optimization algorithm to solve a feasible solution. It mainly contains three parts, constraints, pre-actuation motions and post-actuation motions:\n(1) The constraints are used to ensure the gripper to be in contact with the object and to implicitly define a certain actuation pose. An actuation pose means the key frame that the robot arm manipulate the object, usually represents the moment when the gripper get contact with the object.\n(2) The pre-actuation motions are used to move the gripper to the actuation pose.\n(3) The post-actuation motions are used to complete the task after the actuation pose.\n\nNow that you have given the constraints part and the pre-actuation part of the task solution config, you need to generate 3 different post-actuation motions for task \"close-safe\" following the same config format shown above based on the constraint part that is generated previously. In this part, your task is to generate some motion plans to solve the task beginning from the actuation pose.\n\n=========\nHere is the task description.\n\nclose a safe door\n\n=========\nHere are some examples of the post-actuation part of some task solution configs.\n\n\"\"\"\ntask_name: CloseBox\ncategory_name: Articulated\n\n# Post-actuation motions for the manipulator. \n# Post-actuation motions should be in the format of a python list. Each element in the list should be a string with lower-case and separated by hyphens.\n\n# post-actuation pose list\npost_actuation_motions: [\"move-backward\"] # [pull lid down]\n\n\"\"\"\n\n\"\"\"\ntask_name: OpenLaptop\ncategory_name: Articulated\n\n# Post-actuation motions for the manipulator. \n# Post-actuation motions should be in the format of a python list. Each element in the list should be a string with lower-case and separated by hyphens.\n\n# post-actuation pose list. pushing down\npost_actuation_motions: [\"move-forward\"] # [push lid up]\n\n\"\"\"\n\n\n=====================================\nThe actuation pose of this task \"close-safe\" is visualized in the first uploaded image. You need to start from this moment to generate some motion plans to solve the task. \n\nSome rules of such motion planning are listed here:\n1. Each step in your plan should be chosen from the following motion set: \"move-forward\", \"move-backward\", \"move-left\", \"move-right\", \"move-up\", \"move-down\". Notice that you can not use the gripper finger to grip anything.\n2. The directions in the movement are defined in the robot's coordinate system. The second uploaded image shows the robot's coordinate system.\n    - The left direction is defined as the direction aligned with the green axis in the image, while the right direction is defined as the opposite direction.\n    - The forward direction is defined as the direction aligned with the red axis in the image, while the backward direction is defined as the opposite direction.\n    - The down direction is defined as the direction aligned with the blue axis in the image, while the up direction is defined as the opposite direction.\n3. The robot arm should not collide with the table in the scene during the execution of your plan.\n4. A plan should include at most 3 steps\n5. The post-actuation motion plans of different solutions can be diverse, but their task names should be the same. Do not use terms that you have not seen before. \n6. The output should be in the YAML format with no extra text.\n\n==============================\nPlease output the 3 different configs for task \"close-safe\" in order. Each config should be in the same YAML format as the examples above with no extra text. Each config should be a list of pre-actuation motions in a separated YAML cell."
            },
            {
                "from": "gpt",
                "value": "```yaml\ntask_name: CloseSafe\ncategory_name: Articulated\n\n# Post-actuation motions for the manipulator. \n# Push the safe door towards the closed position.\npost_actuation_motions: [\"move-forward\"]\n```\n\n```yaml\ntask_name: CloseSafe\ncategory_name: Articulated\n\n# Post-actuation motions for the manipulator. \n# Move to the side then push the door forward to close it, to ensure alignment.\npost_actuation_motions: [\"move-right\", \"move-forward\"]\n```\n\n```yaml\ntask_name: CloseSafe\ncategory_name: Articulated\n\n# Post-actuation motions for the manipulator. \n# Move up to avoid touching the table, then push forward to close the door.\npost_actuation_motions: [\"move-up\", \"move-forward\"]\n```"
            }
        ]
    }

]